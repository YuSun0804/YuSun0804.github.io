---
title: A Approach of Image Captioning System
description: The blog is going to build a web system that can automatically generate natural language description for the image. In the whole system, the core component is building an image captioning model to process the image.
categories:
 - NLP
 - CV
 - DL
tags:
---

## Introduction
About image captioning, there are three well-studied approaches to automatic image captioning: retrieval based image captioning, template based image captioning, and deep neural network based image captioning. Moreover, each approach can be divided into some subcategories based on the specific method and framework. In terms of the retrieval based model, Hodosh et al. establish a framework for sentence-based image description by ranking a given pool of captions. Ordonez et al. used 1 Million Captioned Photographs associated with human-generated description to develop an automatic image description method by filtering and retrieving. The advantage of retrieval-based methods is that always return well-formed human-written captions, but in some conditions, these captions may not be able to describe the image properly. Another method is called the template based model, generating caption through a syntactically and semantically constrained process. Yang et al. first extracted nouns, verbs, scenes, and prepositions from the image then used these to generate the sentence by the statistical language model. Li et al. used a similar method, the whole process can be divided into two steps, phrase selection and phrase fusion. Compared with the retrieval based model, the caption generated by this approach is more relevant, but the sentence is usually simple and less creative. Besides, with the development of the neural network, some generation method
like seq-2-seq model has been used to solve this problem, motivated by the neural machine translation. Kiros et al. brought the encoder-decoder framework into image captioning area by unifying joint image-text embedding models through multimodal neural language models. Vinyals et al. used to CNN as encoder and LSTM as decoder for image captioning, by the pre-trained image representation and treating the last hidden layer as input to the decoder. You et al. proposes a new image captioning approach that combines the top-down and bottom-up approaches through a semantic attention model. Automatic image captioning is a relatively new task, the network with encoder and decoder is widely used in this area, currently. Besides, some improvement tricks like attention mechanisms used in other areas like machine translation can also apply in image captioning.


## System Architecture
The whole system contains two main components, including business logic layer and model layer. For the business logic layer, it is mainly used to interact with end-user, and in Model Layer build a generic encoder-decoder model combining some CNN architectures and RNN architectures.

We compared the sequence-to-sequence with the 4 kinds of CNN architectures as the encoders and the 2 kinds of RNN architectures as the decoders.For the CNN encoders, we used 4 kinds of pre-trained iamge processing models, including VGG16,InceptionV3,MobileNet and ResNet, to extract features from images into the hidden layer. For RNN decoders, we tried 2 common used models, including stacked LSTM and GRU with attention mechanism, to generate natural language description of the image from hidden layer. So, we got 8 models in total, which are summarized in the Table. Through the whole model, we can get natural language sentences directly from visual data without any other steps.

| NO. | Encoder | Decoder |
| --- | ------- |-------- |
| 1 | InceptionV3 | GRU with Bahdanau Attention |
| 2 | MobileNetV2 | GRU with Bahdanau Attention |
| 3 | ResNet50 | GRU with Bahdanau Attention |
| 4 | VGG16 | GRU with Bahdanau Attention |
| 5 | InceptionV3 | three stacked LSTM |
| 6 | MobileNetV2 | three stacked LSTM |
| 7 | ResNet50 | three stacked LSTM |
| 8 | VGG16 | three stacked LSTM |

For encoders, we used pre-trained models with transfer learning and deleted the last fully connected layer as the feature extractor. For decoders, attention mechanism in GRU can predict the next caption word based on the image with certain attention areas and previous caption words context. In addition, stacked LSTM can apply deeper structure to predict captions because the deeper model structure, the better prediction performance in deep learning field in most cases.

For image captioning task, encoder can extract features from images with the help of convolutional neural network. These features can work as initial hidden state to feed into the RNN decoder. With the start token, the decoder can predict the next caption word one by one until meeting the end token. In training step, after one word is generated, the RNN will move into the next unit and the input of next unit is the original word in previous unit position from the dataset caption which related to this certain image. In predicting step, after one word is generated, the input of next unit is the predicted word in the previous unit. So there is a small difference between training and predicting steps.

![vm_directory @8x]({{ "/assets/images/post/model.png" | absolute_url }})


## References
1. S. Bai and S. An, “A survey on automatic image caption generation,” Neurocomputing, vol. 311, 05 2018.
2. Y. Yang, C. L. Teo, H. Daumé III, and Y. Aloimonos, “Corpus-guided sentence generation of natural images,” in Proceedings of the Conference on  Empirical Methods in Natural Language Processing, pp. 444–454, Association for Computational Linguistics, 2011.
3. O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural image caption generator,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3156–3164, 2015.
4. Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo, “Image captioning with semantic attention,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4651–4659, 2016.
5. R. Kiros, R. Salakhutdinov, and R. S. Zemel, “Unifying visual-semantic embeddings with multimodal neural language models,” arXiv preprint arXiv:1411.2539, 2014.

